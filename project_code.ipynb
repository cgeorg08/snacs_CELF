{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "from random import uniform, seed\n",
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "budget = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: graph, the set of seed nodes, spread probability, Monte-Carlo simulations --> Out: marginal gain\n",
    "\n",
    "def IC(g,S,p=0.5,mc=1000):\n",
    "\n",
    "    # Loop over the Monte-Carlo Simulations\n",
    "    spread = []\n",
    "    for i in range(mc):\n",
    "        \n",
    "        # Simulate propagation process      \n",
    "        new_active, A = S[:], S[:]\n",
    "        while new_active:\n",
    "\n",
    "            # For each newly active node, find its neighbors that become activated\n",
    "            new_ones = []\n",
    "            for node in new_active:\n",
    "\n",
    "                node_dict = dict(g[node])\n",
    "                node_neighbours = list(node_dict.keys())\n",
    "                \n",
    "                global node_weights_globaldict\n",
    "                weights_list = node_weights_globaldict[node]\n",
    "                weights_np = np.array(weights_list) \n",
    "\n",
    "                # Determine neighbors that become infected\n",
    "                np.random.seed((int)(node)+i)                \n",
    "                success = (np.random.uniform(0,1,len(g[node]))+weights_np) > p\n",
    "                new_ones += list(np.extract(success, node_neighbours))\n",
    "\n",
    "            new_active = list(set(new_ones) - set(A))\n",
    "            \n",
    "            # Add newly activated nodes to the set of activated nodes\n",
    "            A += new_active\n",
    "            \n",
    "        spread.append(len(A))\n",
    "        \n",
    "    return(np.mean(spread))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: graph, #nodes in the set of seed nodes, spread probability, Monte-Carlo simulations, cost dictionary and budget --> Out: set of seed nodes, reward and time\n",
    "\n",
    "def greedy(g,k,p=0.1,mc=1000,node_cost_dict={}, budget=budget):\n",
    "\n",
    "    S, spread, timelapse, start_time = [], [], [], time.time()\n",
    "    \n",
    "    # Find k nodes with largest marginal gain\n",
    "    for _ in tqdm(range(k)):\n",
    "        node = None\n",
    "\n",
    "        # Loop over nodes that are not yet in seed set to find biggest marginal gain\n",
    "        best_spread = 0\n",
    "        # for j in set(range(len(g.nodes)))-set(S):\n",
    "        for j in set(g.nodes)-set(S):\n",
    "\n",
    "            if node_cost_dict[j]<=budget:\n",
    "                # Get the spread\n",
    "                s = IC(g,S + [j],p,mc)/node_cost_dict[j]\n",
    "\n",
    "                # Update the winning node and spread so far\n",
    "                if s > best_spread:\n",
    "                    best_spread, node = s, j\n",
    "                    \n",
    "        if node != None:\n",
    "            # Add the selected node to the seed set\n",
    "            S.append(node)\n",
    "            budget = budget - node_cost_dict[node]\n",
    "\n",
    "            print('**GREEDY HERE** - picked up node:',node)\n",
    "            \n",
    "            # Add estimated spread and elapsed time\n",
    "            spread.append(best_spread)\n",
    "            timelapse.append(time.time() - start_time)\n",
    "\n",
    "    return(S,spread,timelapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: graph, #nodes in the set of seed nodes, dimeter of detection, Monte-Carlo simulations --> Out: set of seed nodes, reward and time\n",
    "\n",
    "def detection_time(g,k,d=10,mc=1000):\n",
    "\n",
    "    nodes_latency_dict = dict()\n",
    "\n",
    "    for root in tqdm(set(g.nodes)):\n",
    "        for i in range(mc):\n",
    "            random.seed((int)(root+i))\n",
    "\n",
    "            previous_node = root\n",
    "            next_node = root\n",
    "            latency_list = list()\n",
    "            current_depth = 1\n",
    "            latency = 0\n",
    "            while current_depth<=d:\n",
    "                node_dict = dict(g[previous_node])\n",
    "                node_neighbours = list(node_dict.keys())\n",
    "                next_node = random.choice(node_neighbours)\n",
    "\n",
    "                key = (previous_node,next_node)\n",
    "                global edge_latency_dict\n",
    "                latency += edge_latency_dict[key]\n",
    "\n",
    "                previous_node = next_node\n",
    "                current_depth+=1\n",
    "            \n",
    "            latency_list.append(latency)\n",
    "            \n",
    "        nodes_latency_dict[root] = np.mean(latency_list)\n",
    "\n",
    "    return(nodes_latency_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: graph, #nodes in the set of seed nodes, spread probability, Monte-Carlo simulations, cost dictionary and budget --> Out: set of seed nodes, reward and time\n",
    "\n",
    "def celf(g,k,p=0.1,mc=1000,node_cost_dict={}, budget=budget):  \n",
    "    \n",
    "    start_time = time.time() \n",
    "    marg_gain = []\n",
    "    for node in g.nodes:\n",
    "        marg_gain.append(IC(g,[node],p,mc)/node_cost_dict[node])\n",
    "\n",
    "    Q = sorted(zip(g.nodes,marg_gain), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "    S, spread, SPREAD = [Q[0][0]], Q[0][1], [Q[0][1]]\n",
    "    Q, LOOKUPS, timelapse = Q[1:], [len(g.nodes)], [time.time()-start_time]\n",
    "    budget = budget - node_cost_dict[Q[0][0]]\n",
    "    \n",
    "    # --------------------\n",
    "    # Find the next k-1 nodes using the list-sorting procedure\n",
    "    # --------------------\n",
    "\n",
    "    for _ in tqdm(range(k-1)):    \n",
    "\n",
    "        check, node_lookup = False, 0\n",
    "\n",
    "        # Shorten the candidate list based on whether the cost of a node is less or equal to our budget\n",
    "        j=0\n",
    "        while j < len(Q):\n",
    "            if node_cost_dict[Q[j][0]] > budget:\n",
    "                Q.pop(j)\n",
    "                j = j-1\n",
    "            j = j+1\n",
    "        \n",
    "        if Q:\n",
    "            while not check:\n",
    "                \n",
    "                # Count the number of times the spread is computed\n",
    "                node_lookup += 1\n",
    "                \n",
    "                # Recalculate spread of top node\n",
    "                current = Q[0][0]\n",
    "                \n",
    "                # Evaluate the spread function and store the marginal gain in the list\n",
    "                marg_gain = (IC(g,S+[current],p,mc)/node_cost_dict[current]) - spread\n",
    "                Q[0] = (current,marg_gain)\n",
    "\n",
    "                # Re-sort the list\n",
    "                Q = sorted(Q, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "                # Check if previous top node stayed on top after the sort\n",
    "                check = (Q[0][0] == current)\n",
    "\n",
    "            # Select the next node\n",
    "            spread += Q[0][1]\n",
    "            S.append(Q[0][0])\n",
    "            budget = budget - node_cost_dict[Q[0][0]]\n",
    "            SPREAD.append(spread)\n",
    "            LOOKUPS.append(node_lookup)\n",
    "            timelapse.append(time.time() - start_time)\n",
    "\n",
    "            # Remove the selected node from the list\n",
    "            Q = Q[1:]\n",
    "\n",
    "    return(S,SPREAD,timelapse,LOOKUPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFile(myfile):\n",
    "    dir = os.getcwd() + '\\\\' + 'datasets'\n",
    "    for f in os.listdir(dir):\n",
    "        if myfile in f and 'data' in f:\n",
    "            return os.getcwd() + '\\\\' + 'datasets' + '\\\\' + f\n",
    "    return 'Error' \n",
    "\n",
    "def readFile(filepath):\n",
    "    filehandler = open(filepath, 'r',encoding='utf-8')\n",
    "    Lines = filehandler.readlines()\n",
    "    return Lines,filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file = 'haggle'\n",
    "lines,filepath = readFile(getFile(target_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = list()\n",
    "edge_weight_dict = dict()\n",
    "\n",
    "if target_file == 'adolescent':\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split(' ')\n",
    "        node1 = (int)(tokens[0])-1\n",
    "        node2 = (int)(tokens[1])-1\n",
    "        edge = (node1,node2)\n",
    "        weight = (float)(tokens[2])\n",
    "\n",
    "        edge_list.append(edge)\n",
    "        edge_weight_dict[edge] = weight\n",
    "\n",
    "elif target_file == 'infectious':\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split(' ')\n",
    "        node1 = (int)(tokens[0])-1\n",
    "        node2 = (int)(tokens[1])-1\n",
    "        edge = (node1,node2)\n",
    "\n",
    "        edge_list.append(edge)\n",
    "\n",
    "    edge_weight_dict = dict(collections.Counter(edge_list))\n",
    "\n",
    "elif target_file == 'haggle':\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split(' ')\n",
    "        node1 = (int)(tokens[0])-1\n",
    "        node2 = (int)(tokens[1].split('\\t')[0])-1\n",
    "        edge = (node1,node2)\n",
    "\n",
    "        edge_list.append(edge)\n",
    "\n",
    "    edge_weight_dict = dict(collections.Counter(edge_list))\n",
    "\n",
    "elif target_file == 'malawi':\n",
    "    for i in range(1,len(lines)):\n",
    "        line = lines[i]\n",
    "        tokens = line.strip().split(',')\n",
    "        node1 = (int)(tokens[3])-1\n",
    "        node2 = (int)(tokens[4])-1\n",
    "        edge = (node1,node2)\n",
    "\n",
    "        edge_list.append(edge)\n",
    "\n",
    "    edge_weight_dict = dict(collections.Counter(edge_list))\n",
    "\n",
    "elif target_file == 'hospital':\n",
    "    mapping_file = os.getcwd() + '\\\\' + 'datasets' + '\\\\hospital_mapping.txt'\n",
    "    # reading the data from the file\n",
    "    with open(mapping_file) as f:\n",
    "        data = f.read()\n",
    "    # reconstructing the data as a dictionary\n",
    "    hospital_dict = ast.literal_eval(data)\n",
    "\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split('\\t')\n",
    "        node1 = hospital_dict[(int)(tokens[1])]\n",
    "        node2 = hospital_dict[(int)(tokens[2])]\n",
    "        edge = (node1,node2)\n",
    "\n",
    "        edge_list.append(edge)\n",
    "\n",
    "    edge_weight_dict = dict(collections.Counter(edge_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the network\n",
    "g = nx.Graph()\n",
    "g.add_edges_from(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find outdegree\n",
    "degree_list = []\n",
    "for k,v in g.degree():\n",
    "    degree_list.append(v)\n",
    "degree_freq_dict = collections.Counter(degree_list)\n",
    "\n",
    "outdegree_values = list(degree_freq_dict.keys())\n",
    "outdegree_freq = list(degree_freq_dict.values())\n",
    "  \n",
    "fig = plt.figure(figsize = (8, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(outdegree_values, outdegree_freq, color ='blue')\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Degree Distribution\")\n",
    "\n",
    "# convert y-axis to Logarithmic scale\n",
    "plt.yscale(\"log\")\n",
    "# plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total number of nodes in the graph = ',len(g.nodes))\n",
    "print('total number of edges in the graph = ',len(g.edges))\n",
    "\n",
    "degree_centrality = nx.degree_centrality(g)\n",
    "\n",
    "# find average degree centrality of the nodes in the network\n",
    "avg_degree_centrality = 0\n",
    "for i in degree_centrality:\n",
    "    avg_degree_centrality += degree_centrality[i]\n",
    "avg_degree_centrality = avg_degree_centrality/len(degree_centrality)\n",
    "print('average degree centrality = ',avg_degree_centrality)\n",
    "print('average degree = ',sum(degree_list)/len(degree_list))\n",
    "\n",
    "print('diameter = ',nx.diameter(g))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSTS \n",
    "- DEFAULT: unit_cost_variable = True ==> costs = 1 for unit-cost algorithms\n",
    "- unit_cost_variable = False ==> costs based on conditions for cost-sensitive algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_cost_variable = True\n",
    "node_cost_dict = dict()\n",
    "if unit_cost_variable:\n",
    "    for i in g.nodes:\n",
    "        node_cost_dict[i] = 1\n",
    "else:\n",
    "    cost_list = [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9]\n",
    "    minimum_degree = min(degree_list)\n",
    "    maximum_degree = max(degree_list)\n",
    "    chunk = (int)((maximum_degree-minimum_degree)/len(cost_list))\n",
    "    for i in g.nodes:\n",
    "        for j in range(len(cost_list)):\n",
    "            starting_point = minimum_degree + j*chunk\n",
    "            if j != len(cost_list)-1:\n",
    "                ending_point = minimum_degree + (j+1)*chunk\n",
    "            else:\n",
    "                ending_point = maximum_degree\n",
    "\n",
    "            if g.degree(i)>=starting_point and g.degree(i)<=ending_point:\n",
    "                node_cost_dict[i] = cost_list[j]\n",
    "                break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEIGHTS\n",
    "- DEFAULT: weights_off_variable = True ==> weights = [0, 0, 0, 0, 0] for non-weight-sensitive algorithms\n",
    "- weights_off_variable = False ==> weights = [-0.2, -0.1, 0, 0.1, 0.2] for weight-sensitive algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_off_variable = True\n",
    "node_neighweights_globaldict = dict()\n",
    "if weights_off_variable:\n",
    "    weights = [0, 0, 0, 0, 0]\n",
    "else:\n",
    "    weights = [-0.2, -0.1, 0, 0.1, 0.2]\n",
    "\n",
    "random.seed(11)\n",
    "for current_node in g.nodes:\n",
    "    neighbours_weight_dict = dict()\n",
    "\n",
    "    tmp_dict = dict(g[current_node])\n",
    "    neighbours = list(tmp_dict.keys())\n",
    "\n",
    "    for neighbour in neighbours:\n",
    "        if neighbour in node_neighweights_globaldict.keys():\n",
    "            num = node_neighweights_globaldict[neighbour][current_node]\n",
    "        else:\n",
    "            num = random.choice(weights)\n",
    "        neighbours_weight_dict[neighbour] = num\n",
    "\n",
    "    node_neighweights_globaldict[current_node] = neighbours_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_weights_globaldict = dict()\n",
    "for k,v in node_neighweights_globaldict.items():\n",
    "    node_weights_globaldict[k] = list(v.values())\n",
    "\n",
    "if target_file != 'infectious':\n",
    "    node_weights_globaldict = edge_weight_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMESTAMPS\n",
    "by default the objective function is population affected but with the code in this section we change the objective function to the detetion time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(11)\n",
    "edge_latency_dict = dict()\n",
    "latencies = [x+1 for x in range(20)]\n",
    "for current_edge in g.edges:\n",
    "    edge_latency_dict[current_edge] = random.choice(latencies)\n",
    "\n",
    "    reverse_edge = (current_edge[1], current_edge[0])\n",
    "    edge_latency_dict[reverse_edge] = edge_latency_dict[current_edge]\n",
    "\n",
    "output = detection_time(g,10,d = 20, mc = 1000)\n",
    "output_top_nodes = dict()\n",
    "for _ in range(10):\n",
    "    Keymax = max(zip(output.values(), output.keys()))[1]\n",
    "    Valuemax = max(zip(output.values(), output.keys()))[0]\n",
    "    output_top_nodes[Keymax] = Valuemax\n",
    "    output.pop(Keymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_centrality = nx.betweenness_centrality(g)\n",
    "outlist = [125,31,49,163,48]\n",
    "\n",
    "# report statistics\n",
    "print('Placement : {} & {} & {} & {} & {}'.format(outlist[0],outlist[1],outlist[2],outlist[3],outlist[4]))\n",
    "print('Deg. centrl. : {:.5f} & {:.5f} & {:.5f} & {:.5f} & {:.5f}'.format(degree_centrality[outlist[0]],degree_centrality[outlist[1]],degree_centrality[outlist[2]],degree_centrality[outlist[3]],degree_centrality[outlist[4]]))\n",
    "print('Betw. centrl. : {:.7f} & {:.7f} & {:.7f} & {:.7f} & {:.7f}'.format(betweenness_centrality[outlist[0]],betweenness_centrality[outlist[1]],betweenness_centrality[outlist[2]],betweenness_centrality[outlist[3]],betweenness_centrality[outlist[4]]))\n",
    "print('Node degree : {} & {} & {} & {} & {}'.format(g.degree(outlist[0]),g.degree(outlist[1]),g.degree(outlist[2]),g.degree(outlist[3]),g.degree(outlist[4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celf_output   = celf(g,5,p = 0.5,mc = 100, node_cost_dict=node_cost_dict, budget=budget)\n",
    "print(\"celf output:   \" + str(celf_output[0]))\n",
    "\n",
    "# save results\n",
    "output_dict = dict()\n",
    "output_dict['celf_output'] = celf_output\n",
    "output_file = os.getcwd() + '\\\\' + 'outputs' + '\\\\' + target_file + '_unitcosts='+str(unit_cost_variable) + '_weightsoff='+str(weights_off_variable)+'.pickle'\n",
    "with open(output_file, 'wb') as handle:\n",
    "    pickle.dump(output_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_centrality = nx.betweenness_centrality(g)\n",
    "outlist = celf_output[0]\n",
    "# report statistics\n",
    "print('Placement = {} & {} & {} & {} & {}'.format(outlist[0],outlist[1],outlist[2],outlist[3],outlist[4]))\n",
    "print('Deg. centrl. = {:.5f} & {:.5f} & {:.5f} & {:.5f} & {:.5f}'.format(degree_centrality[outlist[0]],degree_centrality[outlist[1]],degree_centrality[outlist[2]],degree_centrality[outlist[3]],degree_centrality[outlist[4]]))\n",
    "print('Betw. centrl. = {:.7f} & {:.7f} & {:.7f} & {:.7f} & {:.7f}'.format(betweenness_centrality[outlist[0]],betweenness_centrality[outlist[1]],betweenness_centrality[outlist[2]],betweenness_centrality[outlist[3]],betweenness_centrality[outlist[4]]))\n",
    "print('Node degree = {} & {} & {} & {} & {}'.format(g.degree(outlist[0]),g.degree(outlist[1]),g.degree(outlist[2]),g.degree(outlist[3]),g.degree(outlist[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_output = greedy(g,10,p = 0.5,mc = 100, node_cost_dict=node_cost_dict, budget=budget)\n",
    "print(\"greedy output: \" + str(greedy_output[0]))\n",
    "\n",
    "# save results\n",
    "# output_dict = dict()\n",
    "# output_dict['greedy_output'] = greedy_output\n",
    "# output_file = 'small_greedy_costs.pickle'\n",
    "# with open(output_file, 'wb') as handle:\n",
    "#     pickle.dump(output_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_CT(x, y_greedy, y_celf):\n",
    "     \n",
    "    # first plot with X and Y data\n",
    "    plt.plot(x, y_greedy, linestyle='--', marker='o', color='r', label='GREEDY')\n",
    "    \n",
    "    # second plot with x1 and y1 data\n",
    "    plt.plot(x, y_celf, linestyle='--', marker='o', color='b', label='CELF')\n",
    "    \n",
    "    plt.xlabel(\"Nodes selected\")\n",
    "    plt.ylabel(\"Time in seconds\")\n",
    "    plt.title('Computation time: CELF vs GREEDY')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot time\n",
    "x_axis = [1,2,3,4,5,6,7,8,9,10]\n",
    "plot_CT(x_axis, greedy_output[2], celf_output[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
